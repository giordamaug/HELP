{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Hs Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Gene Essential CEG  Essential OEG\n",
      "0  ENSG00000107581     Essential      Essential\n",
      "1  ENSG00000068654     Essential            NaN\n",
      "2  ENSG00000088325     Essential            NaN\n",
      "4  ENSG00000165732     Essential  Non-essential\n",
      "5  ENSG00000085840     Essential  Non-essential\n",
      "Essential CEG\n",
      "Non-essential    13743\n",
      "Essential          833\n",
      "Name: count, dtype: int64\n",
      "Essential CEG\n",
      "1    13743\n",
      "0      833\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set working directory\n",
    "#os.chdir(\"/to/CLEARER_directory/\")\n",
    "EsInfo = pd.read_csv(\"Class_labels/Hs.csv\", sep=\",\", header=0)\n",
    "EsInfo = EsInfo[EsInfo['Essential CEG'].notna()] # remove rows with NaN in 'Essential CEG'\n",
    "print(EsInfo.head())\n",
    "print(EsInfo['Essential CEG'].value_counts())\n",
    "\n",
    "# Generate class labels suitable for Python\n",
    "EsInfo['Essential CEG'] = EsInfo['Essential CEG'].astype('category').cat.codes\n",
    "EsInfo.set_index('Gene', inplace=True)\n",
    "print(EsInfo['Essential CEG'].value_counts())\n",
    "\n",
    "# Load combined features\n",
    "Data = pd.read_csv(\"Features/Hs_features.csv.gz\", sep=\",\", header=0, compression='gzip')\n",
    "Data.set_index('genes', inplace=True)\n",
    "\n",
    "# reduce to sahred index\n",
    "idxcommon = np.intersect1d(EsInfo.index.values, Data.index.values)\n",
    "Data = Data.loc[idxcommon]\n",
    "EsInfo = EsInfo.loc[idxcommon]\n",
    "\n",
    "# Assign class labels\n",
    "Data['label'] = EsInfo['Essential CEG']\n",
    "\n",
    "# Randomize Data\n",
    "np.random.seed(69)\n",
    "Data = Data.sample(frac=1).reset_index()\n",
    "Data = Data.set_index('genes')\n",
    "\n",
    "X = Data.iloc[:, :-1]\n",
    "Y = Data.iloc[:, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Bio+CCcfs+embed features\n",
    "Load the ENS to symbol convertion dictionary for genes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"../../data\"\n",
    "def get_ens_dict(file_path):\n",
    "    with open(file_path) as f:\n",
    "        gtf = list(f)\n",
    "\n",
    "    gtf = [x for x in gtf if not x.startswith('#')]\n",
    "    gtf = [x for x in gtf if 'gene_id \"' in x and 'gene_name \"' in x]\n",
    "    if len(gtf) == 0:\n",
    "        print('you need to change gene_id \" and gene_name \" formats')\n",
    "    \n",
    "    gtf = list(map(lambda x: (x.split('gene_id \"')[1].split('\"')[0], x.split('gene_name \"')[1].split('\"')[0]), gtf))\n",
    "    gtf = dict(set(gtf))\n",
    "    return gtf\n",
    "\n",
    "gtf_dict = get_ens_dict(os.path.join(path,'Homo_sapiens.GRCh38.112.gtf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essential CEG\n",
      "Non-essential    13734\n",
      "Essential          833\n",
      "Name: count, dtype: int64\n",
      "                   Gene Essential CEG  Essential OEG\n",
      "gene                                                \n",
      "EIF3A   ENSG00000107581     Essential      Essential\n",
      "POLR1A  ENSG00000068654     Essential            NaN\n",
      "TPX2    ENSG00000088325     Essential            NaN\n",
      "DDX21   ENSG00000165732     Essential  Non-essential\n",
      "ORC1    ENSG00000085840     Essential  Non-essential\n",
      "Essential CEG\n",
      "1    13734\n",
      "0      833\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from HELPpy.preprocess.loaders import load_features\n",
    "path = \"../../data\"\n",
    "Data = load_features([os.path.join(path, \"Human_Bio.csv\"),\n",
    "                      os.path.join(path, \"Human_CCcfs.csv\"),\n",
    "                      os.path.join(path, \"Human_EmbN2V_128.csv\")], fixnans=[True, True, False], normalizes=['std', 'std', None])\n",
    "EsInfo = pd.read_csv(\"Class_labels/Hs.csv\", sep=\",\", header=0)\n",
    "EsInfo['gene'] = EsInfo.apply(lambda x: gtf_dict[x.Gene] if x.Gene in gtf_dict.keys() else np.nan, axis=1)\n",
    "EsInfo = EsInfo[EsInfo['gene'].notna()]\n",
    "EsInfo = EsInfo[EsInfo['Essential CEG'].notna()] # remove rows with NaN in 'Essential CEG'\n",
    "EsInfo.set_index('gene', inplace=True)\n",
    "print(EsInfo['Essential CEG'].value_counts())\n",
    "print(EsInfo.head())\n",
    "\n",
    "# Generate class labels suitable for Python\n",
    "EsInfo['Essential CEG'] = EsInfo['Essential CEG'].astype('category').cat.codes\n",
    "print(EsInfo['Essential CEG'].value_counts())\n",
    "\n",
    "idxcommon = np.intersect1d(EsInfo.index.values, Data.index.values)\n",
    "Data = Data.loc[idxcommon]\n",
    "EsInfo = EsInfo.loc[idxcommon]\n",
    "\n",
    "# Assign class labels\n",
    "Data['label'] = EsInfo['Essential CEG']\n",
    "\n",
    "# Randomize Data\n",
    "np.random.seed(69)\n",
    "Data = Data.sample(frac=1).reset_index()\n",
    "Data = Data.set_index('Gene')\n",
    "\n",
    "X = Data.iloc[:, :-1]\n",
    "Y = Data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load reduced Hs Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Gene Essential CEG  Essential OEG\n",
      "0  ENSG00000107581     Essential      Essential\n",
      "1  ENSG00000068654     Essential            NaN\n",
      "2  ENSG00000088325     Essential            NaN\n",
      "4  ENSG00000165732     Essential  Non-essential\n",
      "5  ENSG00000085840     Essential  Non-essential\n",
      "Essential CEG\n",
      "Non-essential    13743\n",
      "Essential          833\n",
      "Name: count, dtype: int64\n",
      "Essential CEG\n",
      "1    13743\n",
      "0      833\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set working directory\n",
    "#os.chdir(\"/to/CLEARER_directory/\")\n",
    "EsInfo = pd.read_csv(\"Class_labels/Hs.csv\", sep=\",\", header=0)\n",
    "EsInfo = EsInfo[EsInfo['Essential CEG'].notna()] # remove rows with NaN in 'Essential CEG'\n",
    "print(EsInfo.head())\n",
    "print(EsInfo['Essential CEG'].value_counts())\n",
    "\n",
    "# Generate class labels suitable for Python\n",
    "EsInfo['Essential CEG'] = EsInfo['Essential CEG'].astype('category').cat.codes\n",
    "EsInfo.set_index('Gene', inplace=True)\n",
    "print(EsInfo['Essential CEG'].value_counts())\n",
    "\n",
    "# Load combined features\n",
    "Data = pd.read_csv(\"Features/Hs_features_lasso.csv\", sep=\",\", header=0)\n",
    "Data.set_index('genes', inplace=True)\n",
    "\n",
    "# reduce to sahred index\n",
    "idxcommon = np.intersect1d(EsInfo.index.values, Data.index.values)\n",
    "Data = Data.loc[idxcommon]\n",
    "EsInfo = EsInfo.loc[idxcommon]\n",
    "\n",
    "# Assign class labels\n",
    "Data['label'] = EsInfo['Essential CEG']\n",
    "\n",
    "# Randomize Data\n",
    "np.random.seed(69)\n",
    "Data = Data.sample(frac=1).reset_index()\n",
    "Data = Data.set_index('genes')\n",
    "\n",
    "X = Data.iloc[:, :-1]\n",
    "Y = Data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply to Hs Feature set or Bio+CCcfs+embed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1}\n",
      "label\n",
      "1    13743\n",
      "0      833\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03cd87bea3d243de8b28654a679177d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "5-fold:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ROC-AUC</th>\n",
       "      <td>0.9730±0.0020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.9281±0.0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BA</th>\n",
       "      <td>0.9094±0.0085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.8883±0.0176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.9305±0.0041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCC</th>\n",
       "      <td>0.5931±0.0137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CM</th>\n",
       "      <td>[[740, 93], [955, 12788]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               measure\n",
       "ROC-AUC                  0.9730±0.0020\n",
       "Accuracy                 0.9281±0.0037\n",
       "BA                       0.9094±0.0085\n",
       "Sensitivity              0.8883±0.0176\n",
       "Specificity              0.9305±0.0041\n",
       "MCC                      0.5931±0.0137\n",
       "CM           [[740, 93], [955, 12788]]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from HELPpy.models.prediction import VotingEnsembleLGBM, k_fold_cv\n",
    "clf = VotingEnsembleLGBM(n_voters=11, learning_rate=0.03, n_estimators=190, boosting_type='gbdt', n_jobs=-1, random_state=42)\n",
    "#clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "df_scores, scores, predictions = k_fold_cv(X, Y, clf, n_splits=5, resample=False, seed=0, show_progress=True, verbose=True)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: Index(['A3s', 'Nc', 'Mitochondrion', 'Cell_membrane', 'Endoplasmic_reticulum',\n",
      "       'Golgi_apparatus', 'Lysosome.Vacuole', 'Peroxisome', 'cgly_count',\n",
      "       'ngly_posmean',\n",
      "       ...\n",
      "       'Pc2.Hydrophilicity.9', 'Pc2.Hydrophobicity.10',\n",
      "       'Pc2.Hydrophilicity.12', 'Pc2.Hydrophilicity.16',\n",
      "       'Pc2.Hydrophobicity.21', 'Pc2.Hydrophobicity.26',\n",
      "       'Pc2.Hydrophilicity.27', 'Pc2.Hydrophilicity.28',\n",
      "       'Pc2.Hydrophilicity.29', 'Pc2.Hydrophobicity.30'],\n",
      "      dtype='object', length=4067)\n",
      "Feature Coefficients: [[ 0.          0.          0.00099024 ... -0.00372794 -0.00397359\n",
      "   0.        ]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature Coefficients:\u001b[39m\u001b[38;5;124m\"\u001b[39m, coefficients)\n\u001b[1;32m      6\u001b[0m X[selected_features]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures/Hs_features_lasso.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mcoefficients\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_feature_indices\u001b[49m\u001b[43m]\u001b[49m, columns\u001b[38;5;241m=\u001b[39mselected_features)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "selected_feature_indices = np.where(model.get_support())[0]\n",
    "selected_features = Data.columns[selected_feature_indices] \n",
    "coefficients = clf.coef_\n",
    "print(\"Selected Features:\", selected_features) \n",
    "print(\"Feature Coefficients:\", coefficients)\n",
    "X[selected_features].to_csv(\"Features/Hs_features_lasso.csv\",index=True)\n",
    "pd.DataFrame(coefficients.ravel()[selected_feature_indices], columns=['lasso_coeff'], \n",
    "             index=pd.Series(name='feature', data=Data.columns[selected_feature_indices])).to_csv(\"Features/Hs_features_lass_coef.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14576, 4067), (14576, 41636))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[selected_features].shape, Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maurizio/miniconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/maurizio/miniconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/maurizio/miniconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/maurizio/miniconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/maurizio/miniconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/maurizio/miniconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/maurizio/miniconda3/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n",
      "/var/folders/gl/gkr8rrn52y9cvb9jc59pvjyw0000gn/T/ipykernel_30683/1283167530.py:12: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Remove highly correlated features\u001b[39;00m\n\u001b[1;32m     11\u001b[0m corr_matrix \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train_selected)\u001b[38;5;241m.\u001b[39mcorr()\u001b[38;5;241m.\u001b[39mabs()\n\u001b[0;32m---> 12\u001b[0m upper \u001b[38;5;241m=\u001b[39m corr_matrix\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39mtriu(np\u001b[38;5;241m.\u001b[39mones(corr_matrix\u001b[38;5;241m.\u001b[39mshape), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m))\n\u001b[1;32m     13\u001b[0m to_drop \u001b[38;5;241m=\u001b[39m [column \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m upper\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(upper[column] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.7\u001b[39m)]\n\u001b[1;32m     14\u001b[0m X_train_selected \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train_selected)\u001b[38;5;241m.\u001b[39mdrop(to_drop, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/numpy/__init__.py:324\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    319\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtesting\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "# Feature selection\n",
    "X_train = X\n",
    "y_train = Y\n",
    "\n",
    "# Lasso feature selection using LogisticRegressionCV\n",
    "clf = LogisticRegressionCV(cv=5, penalty='l1', solver='saga', scoring='roc_auc', max_iter=1000).fit(X_train, y_train)\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_train_selected = model.transform(X_train)\n",
    "\n",
    "# Remove highly correlated features\n",
    "corr_matrix = pd.DataFrame(X_train_selected).corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]\n",
    "X_train_selected = pd.DataFrame(X_train_selected).drop(to_drop, axis=1)\n",
    "\n",
    "train_sets[i] = pd.concat([X_train_selected, y_train.reset_index(drop=True)], axis=1)\n",
    "val_sets[i] = val_sets[i][train_sets[i].columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gl/gkr8rrn52y9cvb9jc59pvjyw0000gn/T/ipykernel_30683/2282695258.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Remove highly correlated features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcorr_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_selected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mupper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorr_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mto_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_train_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_selected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_drop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[1;32m  10703\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10704\u001b[0m         \u001b[0mmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pearson\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10707\u001b[0;31m             \u001b[0mcorrel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnancorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_periods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10708\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"spearman\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10709\u001b[0m             \u001b[0mcorrel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnancorr_spearman\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_periods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10710\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"kendall\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Remove highly correlated features\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "corr_matrix = pd.DataFrame(X_train_selected).corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]\n",
    "X_train_selected_2 = pd.DataFrame(X_train_selected).drop(to_drop, axis=1)\n",
    "\n",
    "train_sets[i] = pd.concat([X_train_selected, y_train.reset_index(drop=True)], axis=1)\n",
    "val_sets[i] = val_sets[i][train_sets[i].columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning\n",
    "def train_rf(train_data):\n",
    "    X = train_data.iloc[:, :-1]\n",
    "    y = train_data.iloc[:, -1]\n",
    "    \n",
    "    smote = SMOTE()\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "    scores = cross_val_score(rf, X_resampled, y_resampled, cv=kf, scoring='roc_auc')\n",
    "    rf.fit(X_resampled, y_resampled)\n",
    "    return rf, scores.mean()\n",
    "\n",
    "results = Parallel(n_jobs=N)(delayed(train_rf)(train_sets[i]) for i in range(N))\n",
    "rf_list, auc_scores = zip(*results)\n",
    "\n",
    "# Performance evaluation on test set\n",
    "def evaluate_model(rf, val_data):\n",
    "    X_val = val_data.iloc[:, :-1]\n",
    "    y_val = val_data.iloc[:, -1]\n",
    "    \n",
    "    y_pred = rf.predict(X_val)\n",
    "    y_prob = rf.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_prob)\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    return cm, roc_auc, pr_auc\n",
    "\n",
    "eval_results = [evaluate_model(rf_list[i], val_sets[i]) for i in range(N)]\n",
    "cm_list, roc_auc_list, pr_auc_list = zip(*eval_results)\n",
    "\n",
    "metrics = pd.DataFrame({\n",
    "    'roc_auc': roc_auc_list,\n",
    "    'pr_auc': pr_auc_list\n",
    "})\n",
    "\n",
    "metrics.loc['mean'] = metrics.mean()\n",
    "metrics.loc['std'] = metrics.std()\n",
    "\n",
    "metrics.to_csv(\"test_rf.csv\", index=True)\n",
    "\n",
    "# Performance evaluation on training set\n",
    "def get_best_kappa(rf):\n",
    "    results = pd.DataFrame(rf.cv_results_)\n",
    "    return results.loc[results['mean_test_score'].idxmax()]\n",
    "\n",
    "train_metrics = pd.concat([get_best_kappa(rf_list[i]) for i in range(5)])\n",
    "train_metrics.to_csv(\"train_rf.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygeometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
